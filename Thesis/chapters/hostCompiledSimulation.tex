\chapter{Host Compiled Simulation}

Host Compiled Simulation is the current focus areas of research in the space of simulation. It is popular because it is simple to understand and develop compared to other approaches, and can provide highly accurate estimates of performance.

\gls{hcs} is based on the approach of Source Code Instrumentation. The source code of the benchmark application is instrumented to collect timing information. The instrumented code is compiled for and run on the Host Machine, hence the name Host Compiled Simulation.

\section{The Approach}
In this research a tool has been developed to automatically instrument the source code of a benchmark application. For accurate instrumentation, the tool analyses the source code along with the cross-compiled binary code generated by the compiler for the target processor.

When a program is executed on a processor, most of the time is spent in executing instructions on the processor pipeline, and fetching data from the memory.

The timing information for time spent in executing instructions in pipeline is annotated at the Basic Block granularity. A Basic Block is a set of instruction which are always executed in a sequence. Each basic block in the binary code is considered. It is assumed that all the data used by the instructions in the basic block is available in L1 Cache, and time spent in fetching data from memory is ignored for now. Each instruction in the basic block simulated to execute on the processor pipeline, and the number of cycles spent is calculated. Cycles spent in pipeline stalls due to interlocking (Control or Data Dependency) among instructions are accounted for. This information is then back-annotated to the corresponding basic block in the source code. Whenever the basic block is executed, the number of cycles spent are accumulated in a global variable. 

Note that at the start of each basic block, it is assumed that the processor pipeline is empty, which is not really the case when Branch Prediction Unit correctly predicts the jump from previous Basic Block to current Basic Block. To accommodate for this, Branch Prediction Unit has been simulated. At the start of each branch, an annotation is added which provides the start address of the branch to the Branch Prediction Emulation Unit which emulates the logic implemented in the target processor to predict branches. If the branch was predicted correctly, a certain number of cycles are subtracted from the total time to remove the penalty which was already added. 

To annotate the timing information for memory access, Cache Hierarchy must be simulated. To do this, each load/store instruction in the binary is identified and mapped to an instruction in the source code. To accurately simulate memory access, we need to reconstruct the memory address for the corresponding variable being fetched on the target system. We then simulate the access to this address in the cache. The cache simulator checks whether the data is present in one of the caches, or should be fetched from the memory. Accordingly, it returns with the number of cycles spent in fetching this data. The result is then accumulated in a global variable.

The annotated code is compiled, and run on the host machine. Accurate timing information is made available from the instrumentation. To calculate the power consumed, Power State Model approach is used. The processor is in a switching (running) state when instructions are being executed in the pipeline, and is in idle state while waiting for data to be fetched from the memory. The power consumed by the processor in each stage in unit time is known. By using this information, total power consumed in running the benchmark application on the target processor can be estimated. Similar approach is used for estimating the power consumed in fetching data from the memory.

\section{Mapping between Source and Binary Code}
It is clear that for correct instrumentation accurate mapping between source code and binary code is very important, but this mapping is generally destroyed by the phases of compiler optimizations. Compilers have multiple



