\chapter{Host Compiled Simulation}

\gls{hcs} is an important area of research in the space of fast simulation techniques for performance estimation. It is popular because it is simple to understand and develop. It is expected to be faster in execution compared to Cycle Accurate Simulators (CAS) and more accurate than other approaches used for Fast Simulation like the Sampling Based Approach.

In this chapter, the concept of \gls{hcs} is illustrated using a simplified example. The steps involved in performing \gls{hcs} are outlined. An overview of each step and the challenges is provided. 

Detailed implementation of the technique and handling of challenges has been discussed in the next chapter.

\section{The Technique}
\gls{hcs} is based on the approach of Source Code Instrumentation (SCI). Instrumentation is the technique to modify the source code of an application, so as to extract useful information during the run-time of the application. In \gls{hcs}, the source code is instrumented to gain information of the time spent in executing the code on a particular Target Processor. The term Target Processor refers to the processor being simulated, and Host Machine is the computer on which the simulation will be run.

When an application is run on a processor, most of the time is spent in following phases.
\begin{itemize} \itemsep -6pt
\item Execution of instructions.
\item Fetching Data from memory, while the execution is stalled.
\end{itemize}

The technique is based on the assumption, that number of cycles spent in each phase of the execution can be accurately predicted using instrumentation. The predicted cycles can be accumulated during the run-time of the application to calculate the total cycles spent in execution of the program. The generated data can further be used to estimate the total power spent in the execution.

\subsection{Simple Example}
This simple example will be able to illustrate the concept of Host Compiled Simulation.

Consider the source code in Listing \ref{lst:sumCCode}. The function \texttt{sum} calculates the sum of elements in an array and returns the result. The object dump from the binary code generated by the ARM cross-compiler is shown in Listing \ref{lst:sumObjCode}. The instrumented code is shown in Listing \ref{lst:sumInstCode}.

\begin{minipage}{0.5\textwidth}
\begin{lstlisting}[caption={Simple C Code},label={lst:sumCCode}]
int sum(int array[20])
{
	int i;
	int sum = 0;
	
	for (i=0; i<20; i++)
		sum += array[i];
	
	return sum;
}
\end{lstlisting}
\end{minipage}%
\begin{minipage}{0.5\textwidth}
\begin{lstlisting}[caption={Objdump Code},label={lst:sumObjCode}]
00008068 <sum>:
8068:     mov     r3, #0
806c:     mov     r2, r3
8070:     ldr     r1, [r0, r3]
8074:     add     r2, r2, r1
8078:     add     r3, r3, #4
807c:     cmp     r3, #80 ; 0x50
8080:     bne     8070 <sum+0x8>
8084:     mov     r0, r2
8088:     bx      lr
\end{lstlisting}
\end{minipage}

\begin{lstlisting}[caption={Instrumented Code},label={lst:sumInstCode}]
unsigned int execCycles;
unsigned int memAccessCycles;

int sum(int array[20])
{
	int i;
	int sum = 0;
	execCycles += 2;										// <<--
	memAccessCycles += simICache(0x8068, 8);				// <<--
	
	for (i=0; i<20; i++)
	{
		sum += array[i];
		memAccessCycles += simDCache(&array + i, READ);			// <<--
		execCycles += 5;									// <<--
		memAccessCycles += simICache(0x8070, 40);			// <<--
	}
	
	execCycles += 2;										// <<--
	memAccessCycles += simICache(0x8084, 8);				// <<--
	return sum;
}
\end{lstlisting}

To accumulate the number of cycles spent in execution two global variables have been declared, \textit{execCycles} and \textit{memAccessCycles}.

From the binary code, 3 basic blocks can be identified. The first basic block starts from line number 2 to 3, the second is a loop from line 4 to 8 and the third between lines 9 and 10. These blocks can easily be matched to corresponding blocks in the source code.

\begin{table}[h]
\begin{center}
\begin{tabularx}{320pt}{>{\centering\arraybackslash}X>{\centering\arraybackslash}X>{\centering\arraybackslash}X>{\centering\arraybackslash}X}
\toprule
	\multicolumn{2}{c}{Basic Block in Binary} & \multicolumn{2}{c}{Matching block in Source}\\ 
	\midrule
	BlockID & Lines & BlockID & Lines \\
    \hline
	1 & 1-2 & 1 & 3-4 \\
	2 & 4-8 & 2 & 7 \\
	3 & 9-10 & 3 & 9 \\	
\bottomrule
\end{tabularx}
\caption{Mapping of Basic Blocks}
\end{center}
\end{table}

For each basic block, we will annotate the time spent in executing the instructions. For simplicity, let us assume that the processor has a single stage pipeline, and each instruction takes 1 cycle to execute. The estimated number of cycles is added to the global variable \textit{execCycles}.

Further, we can see one load operation in the object code on line 4, which corresponds to the loading of elements of the array. To estimate the time spent in loading this data, the cache hierarchy of the target system will be simulated. The cache simulation offers an API \textit{simDCache} to simulate data cache access and takes as parameter the address of the data and a flag to signify whether it is a read or write operation. The cache simulator returns the number of cycles spent in performing the memory access. This value is accumulated in the global variable \textit{memAccessCycles}. 

Also, the instruction cache access must be simulated. This is done at the basic block granularity. The cache simulator offers API \textit{simICache} which takes as parameters \textit{address} of the first instructions in the basic block, and \textit{size} of the basic block in bytes. The cache simulator returns the number of cycles spent in fetching the instructions. The value is also accumulated in global variable \textit{memAccessCycles}.

\section{The Approach}
In this section, a brief overview of the project architecture is provided. Each part of the project is independent, and has been treated as a separate problem. The flow chart in figure [TODO] shows each step of the project. The approach to solve these techniques is presented in the subsequent sections in this chapter. Details of the implementation have been presented in the next chapter.

From the example, it is clear that for correctly instrumenting the code accurate mapping is required between the source code and the binary code. However, this mapping is usually destroyed during the optimization phases of the compiler. The first problem that is solved in this project is to generate accurate mapping information at the Basic Block granularity. 

In the next step, GDB is used to derive information about each variable that is used in the application. During compiler optimization, most of the variables defined by the user are optimized out, hence this information can only be extracted from the debug information with the binary code. Names, addresses and sizes of Global and Local Variables are extracted.

For simulating data cache, each load and store instruction is accurately matched to an instruction in the source code. The variable being accessed is identified, and the annotation to simulate the memory access is added to the code. For simulating the instruction cache, annotation is added for each basic block in the binary code to the corresponding basic block in the source code.

To estimate the amount of cycles spent in execution of a basic block in the processor pipeline, each instruction in a basic block is sequentially parsed to identify interlocking. Interlocking occurs when there is a Control or Data Dependency between instructions, which results in pipeline stall for a few cycles. The cycles used by each basic block are annotated back to the code. Further, a mechanism to emulate Branch Prediction Unit has been implemented. 

\section{Mapping between Source and Binary Code}
The compilers use complex optimization strategies, in which the code is rearranged. Some optimizations techniques used by compilers are Partial or Full Loop Unrolling, and Conditional Execution. These have been briefly described in section \ref{sec:CompilerOptimizations}. Due to the optimizations, the mapping between the source code and binary code is destroyed, and regenerating this mapping is challenging problem.

GDB can be used to map each instruction in the object code to a line in the source code. However this mapping is highly inaccurate, and can not be used. Prominent techniques presented by research papers in the area use Control and Data Flow Analysis to reconstruct the mapping at a Basic Block granularity. In this project, the mapping algorithm described in [TODO] has been used.

The compiler optimizations are performed in two phases. The front-end of the compiler converts the high level source code, into a standard GIMPLE representation. Processor independent optimizations are applied to the GIMPLE code, and the optimized code is known as Intermediate Code. The compiler back-end translates the Intermediate Code to the Machine Language, and applies processor dependent optimization techniques.

It has been observed, that the Control Flow of Intermediate GIMPLE code is closer to the Control Flow of the binary. To simplify mapping, the tool translates the Intermediate GIMPLE code to C Code. The generated C Code is called \gls{isc}. The mapping algorithm then tries to match the Control Flow of the \gls{isc} with that of the cross-compiled binary. The instrumentation is also done on the \gls{isc}.

The Control Flow Graphs are generated by parsing the \gls{isc} and binary code. Each block in the binary code needs to be matched to one or more blocks in the Source Code. To perform this matching a recursive algorithm based on Depth First Traversal has been implemented. Special handling is needed to identify differences in graphs that may occur due to compiler optimization. The algorithm is unable to differentiate between two branches which have the similar structure. Information from GDB is used to perform accurate matching in this case. Detailed description for the algorithm will be covered in section [TODO].

Since each optimization strategy used by the compiler that modifies the control flow needs to be handled specially, only a subset of all optimizations have been handled. The compiler provides a mechanism to allow the developer to configure the level of optimizations to be performed from "\texttt{O0}" to "\texttt{O3}". The tool currently handles optimizations covered in "\texttt{O1}" level. The algorithm can be further extended to handle other optimizations. From tests it has been seen that a significant improvement in performance is achieved by changing optimization level of the compiler from "\texttt{O0}" to "\texttt{O1}", and increasing the optimization level has smaller impact on performance comparatively. 

The algorithm generates error messages, when accurate mapping could not be found. A graphical representation of the \gls{cfg}s is generated, which indicates the mapping generated by the algorithm, to give the user a chance to validate the mapping. The algorithm generates the mapping accurately for the benchmark applications used for testing the tool.

The output generated from this stage contains separate Control Flow Graphs for each function, from the source code and the binary code. It also contains the mapping information. This output is passed to the next stages.

\section{Extracting Debug Information from GDB}
\label{sec:C3GDBInfo}
As discussed briefly in the example, each load/store instruction in the binary code must be matched to a variable in the source code to simulate Data Cache Access. To be able to do this, information about each variable used in the code must be extracted. GDB has been used to extract this information.

Section \ref{sec:GDBInfo} describes how GDB can be used to extract names, addresses and sizes of each Global and Local Variable. GDB also offers a mechanism to use scripts, which operate a series of commands and generate an output, which can be later parsed to gather the information required. This mechanism of GDB has been used.

The tool automatically generates GDB scripts and runs them, to extract information about the variables used in the program. For each variable the following information is collected.

\begin{itemize} \itemsep -6pt
\item \texttt{name} of the variable
\item \texttt{scope} of the variable. Empty for Global Variables, name of the containing function for local variables.
\item \texttt{address} of the variable. For Global Variables it is the physical address. For local variables, it is the address relative to the Stack Pointer.
\item \texttt{type} of the variable.
\item \texttt{size} of the variable.
\end{itemize}

\section{Data Cache Simulation}
To estimate total time spent in fetching data from the memory, the tool performs detailed Cache Simulation. Each load/store instruction in the binary code is matched with to an instruction in the source code. The address from which the data is being fetched is identified, and memory access for the address is simulated. 

Generally speaking, the host and target processors may use a different memory layout. For example, in some processors memory needs to be allocated in an aligned fashion for better performance, however this may not be the case for another processor. Also, the sizes for the basic data types may differ among different architectures. Size of an integer in one architecture may be 4 Bytes and in another may be 2 Bytes. For accurate cache simulation, the address at which the variable resides in the Host Machine, can not be used. Instead, each memory access as it would occur on the target processor must be simulated.

The challenge lies in extracting the address from which the data is being fetched. This address can not be easily extracted from the cross-compiled binary by static analysis. To extract this address, the tool implements a mechanism to reconstruct each memory access as it occurs on the target processor. The approach is based on the research published in [TODO].

An application uses memory to read and write input and output data. For simplicity, let us consider how an application written in C Programming language uses memory. The memory can be used by the application in 3 ways. 

\begin{itemize} \itemsep -6pt
\item \textbf{Global Memory.} Data stored in Global Memory is accessible by all functions in the program. The memory is stored in a fixed size Data Section. Size of each Global Variable must be known at compile time, so it is called statically allocated memory. In bare metal applications, the physical address of each global variable is known after compilation, and can be extracted from the binary.
\item \textbf{Local Memory.} Each function can define variables which can only be used inside the function. The content of local variables is stored in the stack frame of the function. The size of the variable must be known at compile time. The addresses used by the local variables at run-time can not be known by static analysis, however the address relative to the Stack Pointer can be extracted.
\item \textbf{Heap Memory.} Applications can also allocate memory at run-time. The content of this dynamically allocated memory is stored in a special section known as Heap. The heap can grow and contract at run-time. The address and sizes of this type of memory can not be known statically. 
\end{itemize}

The current implementation of the tool, only focuses on simulating access from the Local and Global Memory. Cache Simulation for Heap Memory is complicated, because the memory allocation algorithm used in the target processors have to be emulated to know the exact address where the memory will be allocated. This means, that the tool can only be used with Benchmark applications that do not use Dynamic Memory. This does not limit the importance of this tool since the goal is to simulate for performance analysis, and not functional verification. %TODO: unnecessary last sentence?

The following section explains how each load/store instruction in binary code is mapped to a variable in the source code. Further, the approach to reconstruct the memory access is explained. Implementation of the cache simulator is explained in brief.

\subsection{Memory Access Reconstruction}
The tool parses binary code and emulates each instruction. It maintains the state of registers and updates it, as per the instructions. Branch instructions are ignored. The load/store instructions use register addressing modes to access data. From the content of the register the address of memory being accessed can be known. The memory being accessed may be an array, and this can not be clearly identified at this point.

The addresses of the Global Variables are known. Also, the addresses of Local Variables relative to the Stack Pointer are known. Using this information, the address of the load/store instruction found by emulation can be mapped to one of the variables.

Variables accessed in each basic block of the binary code are recorded, and this information will be used to map the load/store instruction to a specific line in the source code. For each basic block in the binary code, the basic block in the source code is parsed to find the line that causes the memory access. This is needed because the for accessing an array, the index to be added to the base pointer can only be extracted from the source code. 

Once this information is known, the memory address for each access can be reconstructed.

\subsection{Annotation for Data Cache Simulation}
\subsubsection{Global Variables}
For each Global Variable, say "\textit{var}" of any data type, another global variable "\textit{var\_addr}" of type "\textit{unsigned long}" is declared in the instrumented code to store the address where "\textit{var}" will be held in the target memory. This address was extracted using the method described in Section \ref{sec:C3GDBInfo}.

The line in the source code where the global variable is being accessed is identified using the technique presented in Section [TODO]. Instrumentation to simulate cache is added after this line. The Cache Simulator is implemented in C language. It offers the following API to simulate Data Cache Access.

\begin{lstlisting} [frame=none,numbers=none]
/**
 * @brief API Function to simulate Data Cache Access
 * 
 * @param address The address being accessed.
 * @param isReadAccess Flag to indicate type of access.
 * 					     True, if Read Access.
 * 					     False, if Write Access.
 *
 * @return number of cycles spent in performing the memory access.
 */ 
unsigned long long simDCache (unsigned long address, 
							     unsigned int isReadAccess)
\end{lstlisting}

The source code is appropriately instrumented using the above API. For accesses to elements in an array, the index multiplied by the size of the data type is added to the base address. The return value is the number of cycles spent in performing the memory fetch. This value is accumulated in a global variable, in a similar way as shown in the simple example above.

\begin{lstlisting}
int result;
unsigned long result_addr = 0x88ac;					// <--
int input_array[20];
unsigned long input_array_addr = 0x88b0;					// <--

void foo()
{
	
	
}
\end{lstlisting}

\subsubsection{Local Variables}
The approach for simulating access of Local Variables is quite similar to the approach used for global variables. A new local variable is declared for each local variable used in the function, with a suffix "\texttt{\_addr}" and data type "\texttt{unsigned long}". This variable contains the address of the local variable, relative to the current stack pointer. To accurately estimate the physical address where the memory resides, the value of the Stack Pointer is needed.

The stack grows and contracts during the run-time of an application. Whenever a function is called, a stack frame is created and the stack pointer is incremented. The stack frame contains the values of the function parameters, the local variables used in the function and the return address for the function. The size of the stack frame for each function can be extracted from the binary, and the start address of the stack is fixed at compile time. 

To maintain the value of the stack pointer, a global variable "\texttt{CSIM\_SP}" is added to the source code, which is initialized to the start address of the stack. At the beginning of each function, the value of "\texttt{CSIM\_SP}" is incremented by the size of the stack frame for the function. The address of the local variable, relative to the stack pointer is added to "\texttt{CSIM\_SP}", to calculate the physical address of the local variable, as would occur on the target processor.

The memory access is simulated using the same API as above.

\subsubsection{Function Parameters}
A func

\subsubsection{Register Spilling}

\subsection{Implementation of Cache Simulator}

\section{Instruction Cache Simulation}

\section{Annotation for Execution Time in Pipeline}

\section{Annotation for Branch Prediction}








